import time
import requests
from lxml import etree
from multiprocessing import Pool
import sys
sys.getdefaultencoding()

#date = ['01_01', '01_02', '01_03', '01_04', '01_05', '01_06', '01_07', '01_08', '01_09', '01_10', '01_11', '01_12', '01_13', '01_14', '01_15', '01_16', '01_17', '01_18', '01_19', '01_20', '01_21', '01_22', '01_23', '01_24', '01_25', '01_26', '01_27', '01_28', '01_29', '01_30', '02_01', '02_02', '02_03', '02_04', '02_05', '02_06', '02_07', '02_08', '02_09', '02_10', '02_11', '02_12', '02_13', '02_14', '02_15', '02_16', '02_17', '02_18', '02_19', '02_20', '02_21', '02_22', '02_23', '02_24', '02_25', '02_26', '02_27', '02_28', '03_01', '03_02', '03_03', '03_04', '03_05', '03_06', '03_07', '03_08', '03_09', '03_10', '03_11', '03_12', '03_13', '03_14', '03_15', '03_16', '03_17', '03_18', '03_19', '03_20', '03_21', '03_22', '03_23', '03_24', '03_25', '03_26', '03_27', '03_28', '03_29', '03_30', '04_01', '04_02', '04_03', '04_04', '04_05', '04_06', '04_07', '04_08', '04_09', '04_10', '04_11', '04_12', '04_13', '04_14', '04_15', '04_16', '04_17', '04_18', '04_19', '04_20', '04_21', '04_22', '04_23', '04_24', '04_25', '04_26', '04_27', '04_28', '04_29', '04_30', '05_01', '05_02', '05_03', '05_04', '05_05', '05_06', '05_07', '05_08', '05_09', '05_10', '05_11', '05_12', '05_13', '05_14', '05_15', '05_16', '05_17', '05_18', '05_19', '05_20', '05_21', '05_22', '05_23', '05_24', '05_25', '05_26', '05_27', '05_28', '05_29', '05_30', '06_01', '06_02', '06_03', '06_04', '06_05', '06_06', '06_07', '06_08', '06_09', '06_10', '06_11', '06_12', '06_13', '06_14', '06_15', '06_16', '06_17', '06_18', '06_19', '06_20', '06_21', '06_22', '06_23', '06_24', '06_25', '06_26', '06_27', '06_28', '06_29', '06_30', '07_01', '07_02', '07_03', '07_04', '07_05', '07_06', '07_07', '07_08', '07_09', '07_10', '07_11', '07_12', '07_13', '07_14', '07_15', '07_16', '07_17', '07_18', '07_19', '07_20', '07_21', '07_22', '07_23', '07_24', '07_25', '07_26', '07_27', '07_28', '07_29', '07_30', '08_01', '08_02', '08_03', '08_04', '08_05', '08_06', '08_07', '08_08', '08_09', '08_10', '08_11', '08_12', '08_13', '08_14', '08_15', '08_16', '08_17', '08_18', '08_19', '08_20', '08_21', '08_22', '08_23', '08_24', '08_25', '08_26', '08_27', '08_28', '08_29', '08_30', '09_01', '09_02', '09_03', '09_04', '09_05', '09_06', '09_07', '09_08', '09_09', '09_10', '09_11', '09_12', '09_13', '09_14', '09_15', '09_16', '09_17', '09_18', '09_19', '09_20', '09_21', '09_22', '09_23', '09_24', '09_25', '09_26', '09_27', '09_28', '09_29', '09_30', '10_01', '10_02', '10_03', '10_04', '10_05', '10_06', '10_07', '10_08', '10_09', '10_10', '10_11', '10_12', '10_13', '10_14', '10_15', '10_16', '10_17', '10_18', '10_19', '10_20', '10_21', '10_22', '10_23', '10_24', '10_25', '10_26', '10_27', '10_28', '10_29', '10_30', '11_01', '11_02', '11_03', '11_04', '11_05', '11_06', '11_07', '11_08', '11_09', '11_10', '11_11', '11_12', '11_13', '11_14', '11_15', '11_16', '11_17', '11_18', '11_19', '11_20', '11_21', '11_22', '11_23', '11_24', '11_25', '11_26', '11_27', '11_28', '11_29', '11_30', '12_01', '12_02', '12_03', '12_04', '12_05', '12_06', '12_07', '12_08', '12_09', '12_10', '12_11', '12_12', '12_13', '12_14', '12_15', '12_16', '12_17', '12_18', '12_19', '12_20', '12_21', '12_22', '12_23', '12_24', '12_25', '12_26', '12_27', '12_28', '12_29', '12_30']
date = ['01_01', '01_02', '01_03', '01_04', '01_05', '01_06', '01_07', '01_08', '01_09']

def get_url():
    url_list = []
    for i in date:
        url = 'http://www.people.com.cn/GB/24hour/index2016_{}.html'.format(i)
        url_list.append(url)
    return url_list


def get_news_url(url):
    try:
        responses = requests.get(url, timeout=10)
    except:
        return None
    html = etree.HTML(responses.content)
    url_list = html.xpath('//div[@id="Searchresult"]//a/@href')
    news_urls = []
    for i in url_list:
        if 'http' in i:
            news_urls.append(i)
    return news_urls


def get_info(url):
    try:
        r = requests.get(url, timeout=10)
    except:
        return
    # print r.content
    content = etree.HTML(r.content)
    info = content.xpath('//div[@id="rwb_zw"]')[0]
    news = info.xpath('string(.)').strip().encode('utf-8')
    name = int(time.time()*1000)
    with open('F:\people2016\{}.txt'.format(name), 'w') as f:
        f.write(news)


if __name__ == '__main__':
    url_list = get_url()
    p = Pool(processes=100)
    for i in url_list:
        news_urls = get_news_url(i)
        if news_urls == None:
            continue
        for h in news_urls:
            p.apply_async(get_info, args=(h,))
    p.close()
    p.join()